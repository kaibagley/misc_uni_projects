---
title: "Assignment 3"
author: "Kai Bagley - 21984315"
date: "12/10/2021"
output:
  pdf_document:
    fig_caption: yes
fig_width: 8
fig_height: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      tidy = TRUE)
```

```{r}
Sys.setenv("VROOM_CONNECTION_SIZE" = 200000)
```

```{r, results='hide'}
library("ggplot2")
library("ggrepel")
library("patchwork")
library("GGally")
library("MASS")
library("reshape2")
library("dendextend")
library("tidyverse")
library("broom")
library("lubridate")
```

## Question 1


### (a) Consider the 4 linkages given in the lecture slides for agglomerative hierarchical clustering. Which favours outlier detection the most?

With $X_n$ an arbritrary point from cluster $n$, the four linkages given are:


* Complete: Largest distance over all $\Delta(X_1, X_2)$
* Single: Smallest distance over all $\Delta(X_1, X_2)$
* Average: Mean distance over all $\Delta(X_1, X_2)$
* Centroid: Distance between means of cluster 1 and 2.

If we define a detected outlier as a singleton in a dendrogram, then the single linkage will likely result in the most singletons of the four linkages. 

In agglomerative clustering, all elements start off in their own cluster, and progressively coalesce into one large cluster at the end of the clustering process. Each step of this process involves finding a pair of clusters separated by the shortest distance and merging them. When using single-linkage, the algorithm finds one pair of elements (each element in a different cluster) that are closest to one another, and merges the corresponding clusters. When the distance between an outlier and the elements of a cluster is being calculated, the outliers single-linkage distance will be large (by nature of an outlier), and larger than the single-linkage distance between other clusters. This means that it will likely take many steps before the single-linkage distance for the outlier will be small enough to cause it to merge with another cluster, and this arises in the clustering dendrogram as a long singleton.

With complete-linkage, the algorithm still wants the minimum linkage-distance between clusters, but the linkage-distance is measured by the pair of points (each in a different cluster) that has the largest between them. This means that even though the outlier-cluster linkage will be large, it will also be large for other cluster-cluster linkages. Depending on the shape and spread of the clusters, there may be cases where the outlier linkage distance is smaller than the distances between other clusters, and the outlier will be merged before them. This means that outliers in the corresponding dendrogram will not stay as singletons for as long as they will for single linkage, and will look like regular data points.

The average and centroid linkages are in between the extremes of complete- or single-linkage clustering, so they wont be the best, or the worst at outlier detection. 


### (b) In *k*-means clustering, explain what an optimal *k*-cluster arrangement is. In practice, is it possible to always get this arrangement?

Define a distance function $\Delta$, and $k < n$ where $k$ is an arbitrary number of clusters, and $n$ is the number of observations. Assume the observations $\mathbb{X}$ have been placed into $k$ clusters $\mathcal{C}_\nu$ with centroids $\overline{\mathbf{X}}_\nu$, where $\nu \leq k$.

Let $\mathcal{P}$ be an arbitrary *k*-cluster arrangement for $\mathbb{X}$:

\begin{equation}
\mathcal{P} = \mathcal{P}(\mathbb{X}, \Delta, k) = \{\mathcal{C}_\nu : \nu=1, ..., k\}
\end{equation}

Where $W_{\mathcal{P}}$ is the within-cluster variability of $\mathcal{P}$.

A *k*-cluster arrangement of $\mathcal{P}$ is optimal if $W_{\mathcal{P}} \leq W_{\mathcal{P}'}$ for all $\mathcal{P}' \neq \mathcal{P}$, where:

\begin{equation}
W_{\mathcal{P}} = \sum^k_{\nu=1} \sum_{\{\mathbf{X}_i \in \mathcal{C}_\nu\}} \Delta(\mathbf{X}_i, \overline{\mathbf{X}}_\nu)
\end{equation}

It is certainly always possible to get the *k*-clustering optimal arrangement of any dataset. Since there are only a finite maximum number of clusters, $k < n$, we could just calculate $W_{\mathcal{P}}$ for every cluster arrangement, and take the arrangement with the smallest within-cluster variance. *In practice* however, this can and will be very computationally difficult, so it's not usually going to be very practical to check every arrangement. 

Therefore, even though there is always an optimal arrangement, it will not always be practical to find. In practice there is no guarantee of getting the optimal arrangement.


## Question 2

Consider the Dow Jones returns data. Read it into R and select all rows, but only the columns for day 1201 - 2400. Refer to this data as `DJ1201`.

```{r}
DJ1 <- read.csv("../data/DJ30returns.csv")
DJuse <- DJ1[-(1:5),]

DJuse <- 
  mutate(DJuse, Date=as_date(X, format="%d/%Om/%y")) %>%
  select(-X) %>%
  mutate_if(is.character, as.numeric)

DJ1201 <- DJuse[1201:2400,]
```


### (a) Use the 30 stocks of `DJ1201` as observations and calculate the following:

i. What is the size of the data matrix
ii. What is the size of the covariance matrix of these obs.?
iii. What is the rank of the covariance matrix?
iv. What is the value of the largest eigenvalue of the cov. matrix?
v. Discuss the 30th eigenvalue
vi. Show a plot of the eigenvalues and comment on its shape


#### (i) What is the size of the data matrix
\
```{r}
dim(DJ1201)
```

There are 1200 rows, and 31 columns, 30 for the stocks and one for the date.


#### (ii) What is the size of the covariance matrix
\
```{r}
DJ1201.cov <- cov(DJ1201[-NCOL(DJ1201)])
dim(DJ1201.cov)
```

It is a 30x30 matrix, corresponding to the 30 stocks, where the date has been excluded.


#### (iii) What is the rank of the covariance matrix
\
```{r}
Matrix::rankMatrix(DJ1201.cov)[[1]]
```

The rank of the covariance matrix is 30


#### (iv) What is the largest eigenvalue of the covariance matrix
\
```{r}
DJ1201.eigen <- eigen(DJ1201.cov)
DJ1201.eigen$values[1]
```

The largest eigenvalue is $~0.00398$


#### (v) Discuss the 30th eigenvalue
\
```{r}
DJ1201.eigen$values[30]
```

The 30th eigenvalue is the smallest one, as they are ordered. The largest eigenvalue is about 32 times larger. Since the rank of the covariance matrix is 30, and all of the eigenvalues are so small, it seems like there isn't much variance in the principal components of the data.


#### (vi) Plot the eigenvalues and discuss
\
```{r scree2a, fig.cap="\\label{fig:scree2a}Scree plot for the DJ1201 covariance matrix"}
DJ1201.eigenvalues <- data.frame(x=factor(paste0("EV", 1:30), 
                                          levels=paste0("EV", 1:30)), 
                                 eigenvalue=DJ1201.eigen$values)

plt.2a <- ggplot(DJ1201.eigenvalues, aes(x=x, y=eigenvalue), group=1) +
  geom_point(size=3) +
  geom_line(group=1) +
  labs(title="Period 1 scree plot") +
  theme(axis.text.x=element_text(angle=90, vjust=0.5, hjust=1))

print(plt.2a)
```

Above we can see the scree plot for the JD1201 covariance matrix. With the "elbow" at around the fourth eigenvalue, it is safe to say the majority of the variance of the data is explained by the first four eigenvectors/principal components.


### (b) Use the stocks as obs. Cluster the stocks using *k*-means clustering with Euclidean distance, `k=2` and `nstart=25`. What is the size of the two clusters? Which stocks belong to the smaller cluster?
\
```{r}
k <- 2
nstart <- 25

DJstock.clus <- kmeans(t(DJ1201[1:30]), centers=k, nstart=nstart)
DJstock.clus.df <- data.frame(cluster_label=DJstock.clus$cluster, t(DJ1201[1:30]))
```

```{r}
kable1 <- DJstock.clus.df %>% count(cluster_label)

knitr::kable(kable1, booktabs=T, linesep="") %>% kableExtra::kable_styling(latex_options=c("striped", "HOLD_position"))
```

Above we see the sizes of each cluster, with 26 stocks in one cluster, and 4 in the other.

```{r}
# Filthy code for finding the obs. in the cluster of size 4
kable2 <- subset(DJstock.clus.df, cluster_label==kable1[kable1[,"n"]==4,"cluster_label"])["cluster_label"]

knitr::kable(kable2, booktabs=T, linesep="") %>% kableExtra::kable_styling(latex_options=c("striped", "HOLD_position"))
```

Above is the 4 stocks that are in the smaller cluster. These are Hewlett Packard, Intel Corp., IBM and Microsoft, four huge technology companies.


### (c) Use the 1200 daily returns as observations. Cluster them using *k*-means with Euclidean distance, and `nstart=25`. Complete the following:

i. For $k=2, ..., 12$, and for each of the $l=2, ..., k$ clusters, calculate the number of observations and display the results in a cluster table.

ii. For $k=2, ..., 12$, show a plot of the within- and between-cluster variability $W$ and $B$, and the total sum of squares against the index $k$.

iii. Based on parts (i) and (ii), state what is the correct number of clusters and why. Comment on the results.

#### (i) Calculate number of observations and display in a cluster table
\
```{r}
k <- 2:12
nstart <- 25

DJ1201.clusters <- lapply(k, function(i) kmeans(DJ1201[1:30], i, nstart=nstart))
names(DJ1201.clusters) <- paste0("cluster", 2:12)

DJ1201.clus.df <- data.frame()
for (i in k) {
  a <- data.frame(k=i, DJ1201[1:30], clus_label=DJ1201.clusters[[i-1]]$cluster)
  DJ1201.clus.df <- rbind(DJ1201.clus.df, a)
}
```

```{r tab_cluster}
kable3 <- xtabs(~clus_label+k, data=DJ1201.clus.df)

knitr::kable(kable3, booktabs=T, row.names=T, linesep="", caption="\\label{tab:tab_cluster}") %>% kableExtra::kable_styling(latex_options=c("striped", "HOLD_position"))
```

Above is a table showing the observations in each cluster $l$ (leftmost column), for each $k=2, ..., 12$. 

#### (ii) For each $k=2, ..., 12$, show a plot of $W$ and $B$, and the total sum of squares against k
\
```{r}
k <- 2:12

ss <- data.frame(matrix(nrow=0, ncol=3))
for (i in k) {
  cluster <- DJ1201.clusters[[i-1]]
  ss <- rbind(ss, c(cluster$tot.withinss, cluster$betweenss, cluster$totss))
}

colnames(ss) <- c("tot_within_ss", "between_s", "total_ss")
```

```{r varplots, fig.width=5, fig.height=3, fig.cap="\\label{fig:varplots}Total within ss, between ss and total ss"}
plt.2c2 <- ggplot(melt(t(ss)), aes(x=factor(Var2+1), y=value, group=Var1, colour=Var1)) +
  geom_point(size=3) +
  geom_line() +
  labs(title="Variability vs. Number of clusters (k)", x="Number of clusters", y="Variablity")

print(plt.2c2)
```

See Figure \ref{fig:varplots}

#### (iii) State the correct number of clusters
\

To determine the optimal number of clusters in *k*-means clustering, we can find an "elbow" in the plot of total within sum-of-squares, which is shown as the `tot_within_ss` line in Figure \ref{fig:varplots}. It's not a very noticeable elbow, but I'd have to say it's at 5 clusters. In (i), 5 clusters is the last level at which the clusters are relatively large, in 6 and more clusters, there's always a cluster that is much smaller than the others, which is a telltale sign we are overfitting the data to too many clusters. 


## Question 3

Use the 30 stocks from `DJ1201`, same as in Question 2

### (a) Calculate the first two principal components and show a score plot. Compare the score plot with Figures 9.11 and 9.12 in the lecture notes. What is noticeable about the score plot? How does it differ from Figures 9.11 and 9.12?

```{r}
tech <- c("T","HON", "IBM", "HWP", "INTC", "MSFT")

DJ1201.pca <- prcomp(t(DJ1201[1:30]))
DJ1201.pca.scplt <- data.frame(DJ1201.pca$x[,1:2], colour=ifelse(is.element(rownames(DJ1201.pca$x[,1:2]), tech), "tech", "other"))
```

```{r scoreplt, fig.width=5, fig.height=3, fig.cap="\\label{fig:scoreplt}PC1/PC2 scoreplot for the 30 stocks, with tech stocks labelled"}
plt.3a <- ggplot(DJ1201.pca.scplt, aes(x=PC1, y=PC2, colour=colour)) +
  geom_point() +
  geom_text_repel(data=subset(DJ1201.pca.scplt, colour=="tech"), aes(label=rownames(subset(DJ1201.pca.scplt, colour=="tech"))), box.padding=0.5) +
  labs(title="Score plot")

print(plt.3a)
```

In Figure \ref{fig:scoreplt}, we can see that 5 of the 6 tech stocks are well separated from the rest of the points along PC1, which tells us that these 5 stocks are correlated. However, one of the tech stocks is part of the cluster of other stocks, "HON". This is consistent with Figure 9.11 and 9.12 in the lecture notes, where "T" and "HON" sit square in the middle of all the other stocks for all plots but the "third quarter" plot in Figure 9.12.


### (b) Apply agglomerative hierarchical clustering with complete linkage and Euclidean distance. Show the dendrogram.
\

```{r dend, fig.width=8, fig.height=4, fig.cap="\\label{fig:dend}Dendrogram of the 30 stocks"}
tech <- c("T","HON", "IBM", "HWP", "INTC", "MSFT")

DJ1201.hclus <- hclust(dist(t(DJ1201[1:30]), method="euclidean"), method="complete")

DJ1201.dend <- as.dendrogram(DJ1201.hclus)
colours <- as.numeric(factor(ifelse(is.element(rownames(t(DJ1201[1:30])), tech), 1, 0)))
colours <- colours[order.dendrogram(DJ1201.dend)]
labels_colors(DJ1201.dend) <- colours

plot(DJ1201.dend, cex=0.6)
```

See Figure \ref{fig:dend}

### (c) Show a cluster table by levels up to 12 levels. Comment on the dendrogram and table. Compare with Figure 9.13 and Table 9.4 in the lecture notes.

```{r tab_cluster1}
DJ1201.hclus.cut <- cutree(DJ1201.hclus, k=1:12)
DJ1201.hclus.cut.fp <- as.data.frame(DJ1201.hclus.cut) %>% 
  pivot_longer(cols=1:12, names_to="level", values_to="cluster")
conf <- xtabs(~cluster+level, DJ1201.hclus.cut.fp)
kable4 <- conf[,paste(1:12)]

knitr::kable(kable4, booktabs=T, linesep="", row.names=T, caption="\\label{tab:tab_cluster1}") %>% kableExtra::kable_styling(latex_options=c("striped", "HOLD_position"))
```

Let's call "HWP", "IBM", "INTC", "T", "HON" and "MSFT" the "tech" stocks, and all the others the "non-tech" stocks.

Table \ref{tab:tab_cluster1} shows the number of stocks in each cluster. Viewing this table in conjunction with the dendrogram in Figure \ref{fig:dend}, we see that the tech stocks "HWP", "IBM", "INTC" and "MSFT" last all the way to level 2, before finally being merged at level 1. The singleton in level 3 is the tech stock "T", which merges in with the non-tech cluster at level 2, and the tech stock "HON" is well and truly mixed in with the non-tech stocks, being in cluster 4 at level 12.

Compared to Figure 9.13 in the lecture notes shows that it was less successful in clustering tech stock "T" with the rest of the tech stocks. It was clustered with "IBM", but far from the fully separate cluster of "HWP", "INTC" and "MSFT". "HON" was still mixed in with the non-tech stocks.

Based on the fact that 4 tech stocks formed a cluster which merged very late, and even though "T" was in a separate cluster, the clustering analysis on the 30 stocks done here seems to be somewhat better clustering of the data.

### (d) Repeat (b) but with the daily returns as the observations

```{r dend1, fig.width=8, fig.height=4, fig.cap="\\label{fig:dend1}Dendrogram of daily returns of 30 stocks"}
tech <- c("T","HON", "IBM", "HWP", "INTC", "MSFT")

DJ1201.d.hclus <- hclust(dist(DJ1201[1:30], method="euclidean"), method="complete")

DJ1201.d.dend <- as.dendrogram(DJ1201.d.hclus)

plot(DJ1201.d.dend, cex=0.6, leaflab="none")
```

See Figure \ref{fig:dend1}

### (e) Show cluster table up to level 12, similar to Table 9.5 in the lecture notes. Compare (d) and (e) results with Figure 9.14 and Table 9.5.

```{r tab_cluster2}
DJ1201.d.hclus.cut <- cutree(DJ1201.d.hclus, k=1:12)
DJ1201.d.hclus.cut.fp <- as.data.frame(DJ1201.d.hclus.cut) %>% 
  pivot_longer(cols=1:12, names_to="level", values_to="cluster")
conf <- xtabs(~cluster+level, DJ1201.d.hclus.cut.fp)
kable5 <- conf[,paste(1:12)]

knitr::kable(kable5, booktabs=T, linesep="", row.names=T, caption="\\label{tab:tab_cluster2}") %>% kableExtra::kable_styling(latex_options=c("striped", "HOLD_position"))
```

The analysis results (shown in Table \ref{tab:tab_cluster2}) in a small cluster of 4 days in level 2, and as the level increases we end up with half of the clusters being small in size ($n \leq 19$) and the rest being bigger. Since there is so many points, it is a bit hard to interpret the dendrogram in Figure \ref{fig:dend1}, but we can still see the clusters described in Table \ref{tab:tab_cluster2}.

If we have a closer look at the second cluster in level 2, the cluster of size 4, we discover the dates:

```{r}
idx <- row.names(DJ1201.d.hclus.cut)[apply(DJ1201.d.hclus.cut, 1, function(x) x[2]==2)]

DJ1201[rownames(DJ1201) %in% idx,31]
```
These dates match with 4 of the 5 dates mentioned in the lecture notes:

* 1997-10-27: Mini crash in global stock market. DJ drops 554 points.
* 1998-08-31: DJ drops 512 points.
* 2000-03-07: DJ drops 374 points.
* 2000-04-14: DJ drops 778 points.

The other notable date is nowhere to be seen in this analysis, or even the `DJ1201` dataset.

Other than this, the lecture notes' version of this analysis is quite different, with fewer large clusters, and more singletons in their Euclidean-distance analysis than in this assignment's analysis.

This likely means the lecture's analysis is better, since it is better able to show important/eventful dates in the DJ index as singletons or smaller clusters, of which it has more of than my analysis.


### (f) Why are the results above different to those in the lecture notes?
\
The analyses differ due to the fact that my analysis is only of the first 1200 days of the full data, whereas the lecture's analysis is of the full 2527 set of daily returns. This is a huge difference, as in this analysis, singletons and small clusters represent unusual events in the DJ index, and my analysis simply misses out on all of the important events in the rest of the 1327 days after the first 1200.

This is why my analysis doesn't have the last notable date mentioned in the lecture's analysis.

## Question 4

Consider the 13-dimensional wine recognition data. Ignore the first column of the data.

```{r}
wine <- read.csv("../data/wine.csv", header=FALSE)
wine.class <- wine[,1]
wine <- wine[,-1]
```

### (a) For $k = 2, ..., 10$, calculate the cluster arrangement using `nstart=50`. List the within- and between-cluster variabilities.
\
```{r}
k <- 2:10
nstart <- 50

wine.clusters <- lapply(k, function(i) kmeans(wine, i, nstart=nstart))
names(wine.clusters) <- paste0("cluster", k)

wine.var <- do.call(cbind, lapply(k, function(i) c(wine.clusters[[i-1]]$tot.withinss, wine.clusters[[i-1]]$betweenss)))
colnames(wine.var) <- paste0("cluster", k)
rownames(wine.var) <- c("within_var", "between_var")
kable6 <- wine.var %>% round()

knitr::kable(kable6, booktabs=T, linesep="") %>% kableExtra::kable_styling(latex_options=c("striped", "scale_down",  "HOLD_position"))
```

### (c) Show the results of clustering up to $k = 10$ in a table similar to that of Table 9.4 in the lecture notes.
\
```{r}
wine.clus.df <- data.frame()
for (i in k) {
  a <- data.frame(k=i, wine, clus_label=wine.clusters[[i-1]]$cluster)
  wine.clus.df <- rbind(wine.clus.df, a)
}

kable7 <- xtabs(~clus_label+k, data=wine.clus.df)

knitr::kable(kable7, booktabs=T, linesep="", row.names=T, caption="\\label{tab:tab_cluster3}") %>% kableExtra::kable_styling(latex_options=c("striped", "HOLD_position"))
```

See Table \ref{tab:tab_cluster3}

### (c) Compare the cluster table in (b) with that in Q3, Lab 8.
\
Q3 Lab 8 involves the same process as done so far in this question 4. However, the only difference is that the data is scaled in the lab, and raw in this assignment.

Both tables are very similar, with reasonable evenly sized clusters for each level. In the unscaled clustering, we get a much smaller cluster arising in level 6 (cluster 3), and a similar thing happens in the scaled analysis, but in level 7 instead.

The most noticeable difference however, is the fact that the within- and between-cluster sum-of-squares are wildly larger in the unscaled analysis than in the scaled. With within-ss for 2-clusters being $4543750$ for unscaled, and $\approx 1649$ for scaled. This is similar for the between-ss, and is certainly a result of the scaling. The wine data has variables with mean $\approx 0$, variables with mean $\approx 750$ and many in between, and this is of course going to negatively affect the k-means analysis of the data, which uses distance metrics to find clusters.

### (d) For the raw data, calculate the cluster statistics WV, CH and between-cluster variability (Lecture notes, Chapter 9) for $k \leq 10$, and plot the results against index $k$.
\

```{r}
wv <- c()
for (i in 2:(10-1)) {
  val <- wine.var[[1, i-1]] / wine.var[[1, i]]
  wv <- c(wv, val)
}
wv <- c(wv, NA)

ch <- c()
for (i in 2:10) {
  num <- wine.var[[2, i-1]] / (i-1)
  den <- wine.var[[1, i-1]] / (NROW(wine)-i)
  ch <- c(ch, num/den)
}
```

```{r crit, fig.width=5, fig.height=4, fig.cap="\\label{fig:crit}Criterion plots"}
plt.data <- data.frame(wv, ch, bss=unlist(wine.var[2,]))

plt.4d <- ggplot(melt(t(plt.data)), aes(x=Var2, y=value, group=Var1, colour=Var1)) +
  geom_point(size=2) +
  geom_line() +
  scale_y_log10() +
  labs(title="Plot of various statistic criteria vs. number of clusters", y="Criterion, Log10 Scale", x="k-clusters") +
  theme(axis.text.x=element_text(angle=90, vjust=0.5, hjust=1))

print(plt.4d)
```

Figure \ref{fig:crit} shows plots of the WV, CH criteria, and the between-cluster variability (bss) against the number of clusters k. 

### (e) Based off of the results of (b) - (d), select the number of clusters for these data. Give a reason for the choice.
\
Above we have given two criteria for the number of clusters (CH and WV). Optimal number of clusters from CH is given by $\hat{k}_{CH} = \arg\max \text{CH}(k)$, and from WV it is given by $\hat{k}_{WV} = \max\{k : \text{WV}(k) > \tau\}$, where $\tau$ is a chosen threshold, usually $1.2 \leq \tau \leq 1.5$.

$\hat{k}_{CH} = \arg\max \text{CH}(k) = 10$, suggesting the optimal number of clusters is 10, whereas $\hat{k}_{WV} = \max\{k : \text{WV}(k) > 1.2\} = 7$ and $\hat{k}_{WV} = \max\{k : \text{WV}(k) > 1.5\} = 6$. This information is hard to believe, seeing as though the data is actually of 3 classes, and not 6, 7 or 10. In the scaled analysis done in Lab 8, the values for the statistics are $\hat{k}_{CH} = 3$ and $\hat{k}_{WV} = 2 |_{\tau=1.2||1.5}$. This is a bit more believable, Lab 8's analysis also doesn't have some of the criteria weirdness that this analysis has, such as how the CH criterion slowly creeps up as $k$ increases.

As mentioned before, the scaled analysis is the real winner here, but since I have to choose an optimal number of clusters, I will say that 6 is best. CH endlessly increases as $k$ increases, and is therefore not trustworthy.

### (f) The wine data comes from three different cultivars. Use 3-means clustering, and compare the results with the membership of the data to the cultivars. Show the results in an appropriate table and comment.
\
```{r conf_mat}
wine.cm <- data.frame(clus_label=wine.clusters$cluster3$cluster, true=wine.class)

kable8 <- xtabs(~true+clus_label, data=wine.cm)

knitr::kable(kable8, booktabs=T, linesep="", row.names=T, caption="\\label{tab:conf_mat}") %>% kableExtra::kable_styling(latex_options=c("striped", "HOLD_position"))
```

Table \ref{tab:conf_mat} is somewhat similar to a confusion matrix. The 3-means clustering analysis places 13 class 1 wines into cluster 1 and 46 into cluster 2, etc.

Table \ref{tab:conf_mat} shows that the clustering analysis did manage to keep a lot of each class together, with $46/(46+13) = 77.9\%$ of class 1 being placed in the same cluster (cluster 2). $79\%$ (cluster 3) and $60\%$ (cluster 1) of class 2 and class 3 were clustered together in the other 2 clusters. Even though the 3-means clustering analysis on the raw data was certainly not practical, it did manage to come up with 3 clusters each of which contained a majority of distinct classes from the other clusters as stated above.