{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32c9e5e6",
   "metadata": {},
   "source": [
    "# Project 2\n",
    "Kai Bagley - 21984315\n",
    "\n",
    "* Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d0336f",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6b5a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24272054",
   "metadata": {},
   "source": [
    "## Task 2: Multiclass Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98156add",
   "metadata": {},
   "source": [
    "This time, I will be using the `ACTIVITY_CD` labels as classes, and continue to use `NARRATIVE` as the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5a4a79",
   "metadata": {},
   "source": [
    "#### Define `Task2Dataset` class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d99554",
   "metadata": {},
   "source": [
    "I must define a new dataset class, since the column names in the dataframe are all different.\n",
    "\n",
    "The descriptions for many of the below classes are omitted, as they are just the same as in Task 1, and are defined because Task 2 must be in a seperate notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36c8c0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "class Task2Dataset(Dataset):\n",
    "    def __init__(self, train_df, test_df, valid_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train_df (pandas.DataFrame): Training dataset\n",
    "            test_df (pandas.DataFrame): Test dataset\n",
    "            valid_df (pandas.DataFrame): Validation dataset\n",
    "            vectorizer (object): Vectorizer created from dataset\n",
    "        \"\"\"\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        self.train_df = train_df\n",
    "        self.train_size = len(self.train_df)\n",
    "        \n",
    "        self.test_df = test_df\n",
    "        self.test_size = len(self.test_df)\n",
    "        \n",
    "        self.valid_df = valid_df\n",
    "        self.valid_size = len(self.valid_df)\n",
    "        \n",
    "        self.df = train_df.append(test_df).append(valid_df)\n",
    "        \n",
    "        # +2 for end and begin tokens\n",
    "        measure_len = lambda sent: len(sent.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, self.df[\"NARRATIVE\"])) + 2\n",
    "        \n",
    "        self._lookup_dict = {\"train\": (self.train_df, self.train_size),\n",
    "                             \"test\": (self.test_df, self.test_size),\n",
    "                             \"valid\": (self.valid_df, self.valid_size)}\n",
    "        self.set_split(\"train\")\n",
    "        \n",
    "        # Class weights\n",
    "        class_counts = self.train_df[\"ACTIVITY_CD\"].value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.activity_codes.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        freq = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(freq, dtype=torch.float32)\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_make_vectorizer(cls, folder_path, vectorizer):\n",
    "        \"\"\"Load dataset and make a new vectorizer from it\n",
    "        Args:\n",
    "            csv (str): Path to folder containing data CSVs\n",
    "            vectorizer (object): One of the two Vectorizer classes,\n",
    "                OHVectorizer for one hot, or \n",
    "                PEVectorizer for use with pretrained embeddings\n",
    "        Returns:\n",
    "            Instance of Task1Dataset\n",
    "        \"\"\"\n",
    "        train_df = pd.read_csv(folder_path + \"train.csv\")\n",
    "        test_df  = pd.read_csv(folder_path + \"test.csv\")\n",
    "        valid_df = pd.read_csv(folder_path + \"valid.csv\")\n",
    "        \n",
    "        # Lemmatize\n",
    "        for x in [train_df, test_df, valid_df]:\n",
    "            x[\"NARRATIVE\"] = x[\"NARRATIVE\"].apply(lambda x:\" \".join([lemmatizer.lemmatize(w) for w in x.split(\" \")]))\n",
    "        \n",
    "        return cls(train_df, test_df, valid_df, vectorizer.from_dataframe(train_df))\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        \"\"\"Returns vectorizer\"\"\"\n",
    "        return self._vectorizer\n",
    "    \n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\"Selects the chosen dataset\n",
    "        Args: \n",
    "            split (str): Select \"train\", \"test\", \"valid\"\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Allow indexing of dataset\n",
    "        Args:\n",
    "            index (int): Index of desired datapoint\n",
    "        Returns:\n",
    "            Dictionary with datapoint's features and labels\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "        \n",
    "        narr_vector, vec_length = self._vectorizer.vectorize(row[\"NARRATIVE\"], self._max_seq_length)\n",
    "        activity_idx = self._vectorizer.activity_codes.lookup_token(row[\"ACTIVITY_CD\"])\n",
    "        return {\"features\":narr_vector,\n",
    "                \"labels\":activity_idx,\n",
    "                \"feat_length\":vec_length}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Return number of batches in dataset from a given batch size\n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            Number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d9c3814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def gen_batches(dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"Generator function, wraps PyTorch DataLoader and ensures \n",
    "    each tensor is in the right device\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    \n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06b421b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Processes text and extracts vocab\"\"\"\n",
    "    def __init__(self, tok_to_idx=None):\n",
    "        \"\"\"Args:\n",
    "            tok_to_idx (dict): Dictionary that maps tokens to indices\n",
    "        \"\"\"\n",
    "        if tok_to_idx is None:\n",
    "            tok_to_idx = {}\n",
    "        self._tok_to_idx = tok_to_idx\n",
    "        self._idx_to_tok = {idx:token for token, idx in self._tok_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {\"tok_to_idx\":self._tok_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, dictionary):\n",
    "        \"\"\"Creates a Vocabulary from a serialized dict\"\"\"\n",
    "        return cls(**dictionary)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update Vocabulary with a new token\n",
    "\n",
    "        Args:\n",
    "            token (str): The token to add to the Vocabulary\n",
    "        Returns:\n",
    "            index (int): Integer index corresponding to the token \n",
    "        \"\"\"\n",
    "\n",
    "        if token in self._tok_to_idx:\n",
    "            index = self._tok_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._tok_to_idx)\n",
    "            self._tok_to_idx[token] = index\n",
    "            self._idx_to_tok[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        \"\"\"Updates Vocabulary with multiple tokens\n",
    "\n",
    "        Args: \n",
    "            tokens (list): List of tokens (str)s\n",
    "        Returns:\n",
    "            indices (list): List of indices (int)s\n",
    "        \"\"\"\n",
    "\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Returns index of a token\n",
    "\n",
    "        Args:\n",
    "            token (str): Token to to find index for\n",
    "        Returns:\n",
    "            index (int): The index of the token\n",
    "        \"\"\"\n",
    "        return self._tok_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Returns token at index\n",
    "\n",
    "        Args:\n",
    "            index (int): Index to search for\n",
    "        Returns:\n",
    "            token (str): Associated token\n",
    "        \"\"\"\n",
    "\n",
    "        if index not in self._idx_to_tok:\n",
    "            raise KeyError(f\"Index ({index}) is not in the Vocabulary\")\n",
    "        return self._idx_to_tok[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<Vocabulary(size={len(self)})>\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._tok_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "772e61bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below Vocabulary object is from lab 9\n",
    "\n",
    "class SequenceVocabulary(Vocabulary):\n",
    "    \"\"\"Processes text and extracts vocab, for sequences\"\"\"\n",
    "    def __init__(self, add_unk=True, tok_to_idx=None, unk_token=\"<UNK>\", mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\", end_seq_token=\"<END>\"):\n",
    "        \"\"\"Args:\n",
    "            tok_to_idx (dict): Dictionary that maps tokens to indices\n",
    "            unk_token (str): The UNK token that will be added to the Vocabulary\n",
    "            mask_token (str): Used as padding for embedding\n",
    "            begin_seq_token (str): Start of a sequence\n",
    "            end_seq_token (str): End of a sequence\n",
    "        \"\"\"\n",
    "        super(SequenceVocabulary, self).__init__(tok_to_idx)\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        self._mask_token = mask_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "        \n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        return {\"tok_to_idx\":self._tok_to_idx,\n",
    "                \"unk_token\":self._unk_token,\n",
    "                \"mask_token\":self._mask_token,\n",
    "                \"begin_seq_token\":self._begin_seq_token,\n",
    "                \"end_seq_token\":self._end_seq_token}\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Returns index of a token, or <UNK> index if not present\n",
    "\n",
    "        Args:\n",
    "            token (str): Token to to find index for\n",
    "        Returns:\n",
    "            index (int): The index of the token\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._tok_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._tok_to_idx[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2814534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "class Task2Vectorizer(object):\n",
    "    \"\"\"Vectorizer for Task 2\"\"\"\n",
    "    def __init__(self, narrative_vocab, activity_codes):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            narrative_vocab (Vocabulary): Vocab for the narrative feature\n",
    "            activity_codes (Vocabulary): Injuries are already encoded as integers\n",
    "        \"\"\"\n",
    "        self.narrative_vocab = narrative_vocab\n",
    "        self.activity_codes = activity_codes\n",
    "    \n",
    "    def vectorize(self, narrative, vec_length=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            narrative (str): The space-separated narrative\n",
    "            vec_length (int): Fix length of vector\n",
    "        Returns:\n",
    "            out (np.ndarray): Vectorized narrative\n",
    "        \"\"\"\n",
    "        idx = [self.narrative_vocab.begin_seq_index]\n",
    "        idx.extend(self.narrative_vocab.lookup_token(token) for token in narrative.split(\" \"))\n",
    "        idx.append(self.narrative_vocab.end_seq_index)\n",
    "        \n",
    "        if vec_length < 0:\n",
    "            vec_length = len(idx)\n",
    "            \n",
    "        out = np.zeros(vec_length, dtype=np.int64)\n",
    "        out[:len(idx)] = idx\n",
    "        out[len(idx):] = self.narrative_vocab.mask_index\n",
    "        \n",
    "        return out, len(idx)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df, cutoff=0):\n",
    "        \"\"\"Create a vectorizer from a dataframe\n",
    "        \n",
    "        Args:\n",
    "            df (pandas.DataFrame): Dataframe\n",
    "        Returns:\n",
    "            Task2Vectorizer object\n",
    "        \"\"\"\n",
    "        word_counter = Counter()\n",
    "        narrative_vocab = SequenceVocabulary(add_unk=True)\n",
    "        for narrative in sorted(set(df[\"NARRATIVE\"])):\n",
    "            for token in narrative.split(\" \"):\n",
    "                if token not in string.punctuation:\n",
    "                    word_counter[token] += 1\n",
    "                    \n",
    "        for word, count in word_counter.items():\n",
    "            if count > cutoff:\n",
    "                narrative_vocab.add_token(word)\n",
    "        \n",
    "        activity_codes = Vocabulary()\n",
    "        for code in sorted(set(df[\"ACTIVITY_CD\"])):\n",
    "            activity_codes.add_token(code)\n",
    "        \n",
    "        return cls(narrative_vocab, activity_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41cca87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_gather(outputs, input_lengths):\n",
    "    '''Get a vector from each batch datapoint in outputs \n",
    "\n",
    "    Args:\n",
    "        outputs (torch.FloatTensor, torch.cuda.FloatTensor)\n",
    "            shape: (batch, sequence, feature)\n",
    "        input_lengths (torch.LongTensor, torch.cuda.LongTensor)\n",
    "            shape: (batch,)\n",
    "\n",
    "    Returns:\n",
    "        outputs (torch.FloatTensor, torch.cuda.FloatTensor)\n",
    "            shape: (batch, feature)\n",
    "    '''\n",
    "    input_lengths = input_lengths.long().detach().cpu().numpy() - 1\n",
    "\n",
    "    out = []\n",
    "    for batch_index, column_index in enumerate(input_lengths):\n",
    "        out.append(outputs[batch_index, column_index])\n",
    "\n",
    "    return torch.stack(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c301253a",
   "metadata": {},
   "source": [
    "### Define the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075ad890",
   "metadata": {},
   "source": [
    "Below I will define a Elman RNN cell, which will be more or less copied from the lab notes. This will be the main part of the classifier, which uses embeddings, fed into the RNN cell, fed into a dense later, which is fully connected to a dense layer as the output.\n",
    "\n",
    "Inputs into the RNN will be word vectors from the dataset. These will be embedded using an embedding function within the RNN classfier. These embedded word vectors are run through a series of cells, in this case the RNN cell first, then two dense layers, with the second being the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23e2a217",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmanRNN(nn.Module):\n",
    "    \"\"\"Elman RNN using RNNCell\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, batch_first=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size (int): size of input vector\n",
    "            hidden_size (int): size of the hidden states\n",
    "            batch_first (bool): true if 0th dim is batch\n",
    "        \"\"\"\n",
    "        super(ElmanRNN, self).__init__()\n",
    "        \n",
    "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
    "        \n",
    "        self.batch_first = batch_first\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def _initial_hidden(self, batch_size):\n",
    "        return torch.zeros((batch_size, self.hidden_size))\n",
    "    \n",
    "    def forward(self, inputs, initial_hidden=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs (torch.Tensor): input data tensor,\n",
    "                if self.batch_first: \n",
    "                    inputs.shape = (batch, seq_size, feature_size)\n",
    "                else: \n",
    "                    inputs.shape = (seq_size, batch, feature_size)\n",
    "            initial_hidden (torch.Tensor): the initial RNN hidden state\n",
    "            \n",
    "        Returns:\n",
    "            hiddens (torch.Tensor): The outputs of the RNN at each step\n",
    "                hiddens.shape = inputs.shape\n",
    "        \"\"\"\n",
    "        if self.batch_first:\n",
    "            batch_size, seq_size, feat_size = inputs.size()\n",
    "            x = inputs.permute(1, 0, 2)\n",
    "        else:\n",
    "            seq_size, batch_size, feat_size = inputs.size()\n",
    "            \n",
    "        hiddens = []\n",
    "        \n",
    "        if initial_hidden is None:\n",
    "            initial_hidden = self._initial_hidden(batch_size)\n",
    "            initial_hidden = initial_hidden.to(x.device)\n",
    "            \n",
    "        hidden_t = initial_hidden\n",
    "        \n",
    "        for t in range(seq_size):\n",
    "            hidden_t = self.rnn_cell(x[t], hidden_t)\n",
    "            hiddens.append(hidden_t)\n",
    "            \n",
    "        hiddens = torch.stack(hiddens)\n",
    "        \n",
    "        if self.batch_first:\n",
    "            hiddens = hiddens.permute(1, 0, 2)\n",
    "        \n",
    "        return hiddens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ad9ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NarrRNNClf(nn.Module):\n",
    "    \"\"\"Narrative classifier using RNN\"\"\"\n",
    "    def __init__(self, embedding_size, num_embeddings, num_classes, rnn_hidden_size, batch_first=True, padding_idx=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_size (int): The size of embeddings\n",
    "            num_embeddings (int): The number of chars to embed\n",
    "            num_classes (int): The number of classes\n",
    "            rnn_hidden_size (int): Size of RNN hidden state\n",
    "            batch_first (bool): True batch is 0th dim\n",
    "            padding_idx (int): Index for tensor pad        \n",
    "        \"\"\"\n",
    "        super(NarrRNNClf, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(num_embeddings=num_embeddings,\n",
    "                                embedding_dim=embedding_size,\n",
    "                                padding_idx=padding_idx)\n",
    "        self.rnn = ElmanRNN(input_size=embedding_size,\n",
    "                            hidden_size=rnn_hidden_size,\n",
    "                            batch_first=batch_first)\n",
    "        self.dense1 = nn.Linear(in_features=rnn_hidden_size,\n",
    "                                out_features=rnn_hidden_size)\n",
    "        self.dense2 = nn.Linear(in_features=rnn_hidden_size,\n",
    "                                out_features=num_classes)\n",
    "        self.relu    = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, inputs, in_lengths=None, apply_softmax=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs (torch.Tensor): input data tensor\n",
    "                inputs.shape = (batch, input_dim)\n",
    "            in_lengths (torch.Tensor): lengths of each seq in batch\n",
    "            apply_softmax (bool): true if use softmax activation.\n",
    "        Returns:\n",
    "            output tensor, tensor.shape = (batch, output_dim)\n",
    "        \"\"\"\n",
    "        x_emb = self.emb(inputs)\n",
    "        x = self.rnn(x_emb)\n",
    "        \n",
    "        if in_lengths is not None:\n",
    "            x = column_gather(x, in_lengths)\n",
    "        else:\n",
    "            x = x[:, -1, :]\n",
    "            \n",
    "        x = self.dropout(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d6b05",
   "metadata": {},
   "source": [
    "### GRU Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0f6558",
   "metadata": {},
   "source": [
    "While I'm at it, I will also define the GRU classfier for the second part of Task 2. This just uses the PyTorch `nn.GRU` class as its main part.\n",
    "\n",
    "Very similar to the above RNN classifier, it simply takes word vectors which are calculated in the `Task2Dataset` object, then embedded within the classifier `NarrGRUClf`. These are fed into a GRU cell, then into 1 dense layer as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "581d93b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NarrGRUClf(nn.Module):\n",
    "    \"\"\"Simple GRU nn\"\"\"\n",
    "    def __init__(self, embedding_size, num_embeddings, hidden_size, num_classes, batch_first=True, padding_idx=0):\n",
    "        super(NarrGRUClf, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(num_embeddings=num_embeddings,\n",
    "                                embedding_dim=embedding_size,\n",
    "                                padding_idx=padding_idx)\n",
    "        self.gru = nn.GRU(input_size=embedding_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=1,\n",
    "                          batch_first=batch_first)\n",
    "        self.dense1 = nn.Linear(in_features=hidden_size,\n",
    "                                out_features=num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, inputs, apply_softmax=False):\n",
    "        x = self.emb(inputs)\n",
    "        x, _ = self.gru(x)\n",
    "        \n",
    "        batch_size, seq_size, feat_size = x.shape\n",
    "        x = x.contiguous().view(batch_size*seq_size, feat_size)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.dense1(x)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            x = self.softmax(x)\n",
    "        \n",
    "        new_feat_size = x.shape[-1]\n",
    "        x = x.view(batch_size, seq_size, new_feat_size)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcb76e9",
   "metadata": {},
   "source": [
    "Redefine the arguments namespace once again for the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dc4414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "args = Namespace(\n",
    "    frequency_cutoff=0,\n",
    "    narrative_folder_path=\"./data/task2/\",\n",
    "    model_state_file=\"./data/task2/RNNmodel.pth\",\n",
    "    vectorizer_file=\"./data/task2/RNNvectorizer.json\",\n",
    "    save_dir=\"./data/task2/\",\n",
    "    embedding_size=100,\n",
    "    rnn_hidden_size=100,\n",
    "    num_epochs=100,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=64,\n",
    "    early_stopping_criteria=5,\n",
    "    cuda=True\n",
    ")\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bdfb004",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Task2Dataset.load_dataset_make_vectorizer(args.narrative_folder_path, Task2Vectorizer)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "clf = NarrRNNClf(embedding_size=args.embedding_size,\n",
    "                 num_embeddings=len(vectorizer.narrative_vocab),\n",
    "                 rnn_hidden_size=args.rnn_hidden_size,\n",
    "                 num_classes=len(vectorizer.activity_codes),\n",
    "                 padding_idx=vectorizer.narrative_vocab.mask_index)\n",
    "\n",
    "clf = clf.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7fec6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def make_train_state(args):\n",
    "    return {\"epoch_index\": 0,\n",
    "            \"train_loss\": [],\n",
    "            \"train_acc\": [],\n",
    "            \"valid_loss\": [],\n",
    "            \"valid_acc\": [],\n",
    "            \"test_loss\": 1,\n",
    "            \"test_acc\": 1,\n",
    "            \"learning_rate\":args.learning_rate,\n",
    "            \"stop_early\":False,\n",
    "            \"early_stopping_step\":0,\n",
    "            \"early_stopping_best_valid\": 1e8,\n",
    "            \"model_filename\": args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Implements early stopping\n",
    "    Args:\n",
    "        args (Namespace): Main model arguments\n",
    "        model (nn.Module): Model\n",
    "        train_state (dict): Current training state\n",
    "    \"\"\"\n",
    "    # Save first model\n",
    "    if train_state[\"epoch_index\"] == 0:\n",
    "        torch.save(model.state_dict(), train_state[\"model_filename\"])\n",
    "        train_state[\"stop_early\"] = False\n",
    "    \n",
    "    # Save model if performance increase\n",
    "    if train_state[\"epoch_index\"] >= 1:\n",
    "        _, loss = train_state[\"valid_loss\"][-2:]\n",
    "        \n",
    "        # If loss increased\n",
    "        if loss >= train_state[\"early_stopping_best_valid\"]:\n",
    "            train_state[\"early_stopping_step\"] += 1\n",
    "        # If loss decreased\n",
    "        else:\n",
    "            # Save best model\n",
    "            if loss < train_state[\"early_stopping_best_valid\"]:\n",
    "                torch.save(model.state_dict(), train_state[\"model_filename\"])\n",
    "            \n",
    "            train_state[\"early_stopping_step\"] = 0\n",
    "            \n",
    "        train_state[\"stop_early\"] = train_state[\"early_stopping_step\"] >= args.early_stopping_criteria\n",
    "        \n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_targ):\n",
    "    _, y_pred_idx = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_idx, y_targ).sum().item()\n",
    "    return n_correct / len(y_pred_idx) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50949d58",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a41f1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03499c4207d4f3dbe6880db97559d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f46b696ffb4830817e90e37c0d0f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Split:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_f    = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(clf.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm(desc=\"Epoch\",\n",
    "                 total=args.num_epochs,\n",
    "                 position=0)\n",
    "\n",
    "dataset.set_split(\"train\")\n",
    "train_bar = tqdm(desc=\"Train Split\",\n",
    "                 total=dataset.get_num_batches(args.batch_size),\n",
    "                 position=1,\n",
    "                 leave=True)\n",
    "\n",
    "for epoch_index in range(args.num_epochs):\n",
    "    train_state[\"epoch_index\"] = epoch_index\n",
    "    \n",
    "    # Training dataset\n",
    "    dataset.set_split(\"train\")\n",
    "    batch_gen = gen_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    clf.train()\n",
    "    for batch_index, batch_dict in enumerate(batch_gen):\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute output\n",
    "        y_pred = clf(inputs=batch_dict[\"features\"], \n",
    "                     in_lengths=batch_dict[\"feat_length\"])\n",
    "        # Compute loss\n",
    "        loss = loss_f(y_pred, batch_dict[\"labels\"])\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch-running_loss) / (batch_index+1)\n",
    "        # Compute gradients using loss\n",
    "        loss.backward()\n",
    "        # Use optimizer to take gradient step\n",
    "        optimizer.step()\n",
    "        # Compute accuracy\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict[\"labels\"])\n",
    "        running_acc += (acc_batch-running_acc) / (batch_index+1)\n",
    "        \n",
    "        # Update the bar\n",
    "        train_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "        train_bar.update()\n",
    "            \n",
    "    train_state[\"train_loss\"].append(running_loss)\n",
    "    train_state[\"train_acc\"].append(running_acc)\n",
    "    \n",
    "    # Validation dataset\n",
    "    dataset.set_split(\"valid\")\n",
    "    batch_gen = gen_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    clf.eval()\n",
    "    for batch_index, batch_dict in enumerate(batch_gen):\n",
    "        # Compute output\n",
    "        y_pred = clf(inputs=batch_dict[\"features\"],\n",
    "                     in_lengths=batch_dict[\"feat_length\"])\n",
    "        # Compute loss\n",
    "        loss = loss_f(y_pred, batch_dict[\"labels\"])\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch-running_loss) / (batch_index+1)\n",
    "        # Compute accuracy\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict[\"labels\"])\n",
    "        running_acc += (acc_batch-running_acc) / (batch_index+1)\n",
    "        \n",
    "    train_state[\"valid_loss\"].append(running_loss)\n",
    "    train_state[\"valid_acc\"].append(running_acc)\n",
    "    \n",
    "    train_state = update_train_state(args=args, model=clf, train_state=train_state)\n",
    "\n",
    "    scheduler.step(train_state['valid_loss'][-1])\n",
    "\n",
    "    train_bar.n = 0\n",
    "    epoch_bar.update()\n",
    "    \n",
    "    if train_state['stop_early']:\n",
    "        break\n",
    "train_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "train_bar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a91c0f7",
   "metadata": {},
   "source": [
    "#### Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4b2f816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "dataset.set_split(\"test\")\n",
    "batch_gen = gen_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "running_loss = 0.0\n",
    "running_acc = 0.0\n",
    "clf.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_gen):\n",
    "    # Compute output\n",
    "    y_pred = clf(inputs=batch_dict[\"features\"], \n",
    "                 in_lengths=batch_dict[\"feat_length\"])\n",
    "    # Compute loss\n",
    "    loss = loss_f(y_pred, batch_dict[\"labels\"])\n",
    "    loss_batch = loss.item()\n",
    "    running_loss += (loss_batch-running_loss) / (batch_index+1)\n",
    "    # Compute accuracy\n",
    "    acc_batch = compute_accuracy(y_pred, batch_dict[\"labels\"])\n",
    "    running_acc += (acc_batch-running_acc) / (batch_index+1)\n",
    "\n",
    "train_state[\"test_loss\"] = running_loss\n",
    "train_state[\"test_acc\"] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb9b12ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 3.515625\n",
      "Test loss    : 2.8411765694618225\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test accuracy: {train_state['test_acc']}\")\n",
    "print(f\"Test loss    : {train_state['test_loss']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc489e9",
   "metadata": {},
   "source": [
    "#### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf436d6a",
   "metadata": {},
   "source": [
    "Clearly the RNN defined here is a terrible example of an RNN, as the purpose of an RNN is to run very well on sequences, which is what sentences/documents are! But due to time constraints, I was unable to troubleshoot this stage too much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2535cc70",
   "metadata": {},
   "source": [
    "### GRU classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22573930",
   "metadata": {},
   "source": [
    "Define the classifier, dataset etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25a0d1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Task2Dataset.load_dataset_make_vectorizer(args.narrative_folder_path, Task2Vectorizer)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "clf = NarrGRUClf(embedding_size=args.embedding_size,\n",
    "                 num_embeddings=len(vectorizer.narrative_vocab),\n",
    "                 hidden_size=args.rnn_hidden_size,\n",
    "                 num_classes=len(vectorizer.activity_codes),\n",
    "                 padding_idx=vectorizer.narrative_vocab.mask_index)\n",
    "\n",
    "clf = clf.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd7e1a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sizes(y_pred, y_true):\n",
    "    \"\"\"Normalize tensor sizes\n",
    "    \n",
    "    Args:\n",
    "        y_pred (torch.Tensor): the output of the model\n",
    "            If a 3-dimensional tensor, reshapes to a matrix\n",
    "        y_true (torch.Tensor): the target predictions\n",
    "            If a matrix, reshapes to be a vector\n",
    "    \"\"\"\n",
    "    if len(y_pred.size()) == 3:\n",
    "#         y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\n",
    "        y_pred = y_pred.contiguous().view(y_pred.size(0),-1)\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true = y_true.contiguous().view(-1)\n",
    "    return y_pred, y_true\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    \n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "    \n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "    \n",
    "    return n_correct / n_valid*100\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91f03e7",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b723dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ec12c5cd104afda3040efb653a899e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac8a5bc50b14115b7acd039f8f3ad5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Split:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "mask_index = vectorizer.narrative_vocab.mask_index\n",
    "optimizer = optim.Adam(clf.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm(desc=\"Epoch\",\n",
    "                 total=args.num_epochs,\n",
    "                 position=0)\n",
    "\n",
    "dataset.set_split(\"train\")\n",
    "train_bar = tqdm(desc=\"Train Split\",\n",
    "                 total=dataset.get_num_batches(args.batch_size),\n",
    "                 position=1,\n",
    "                 leave=True)\n",
    "\n",
    "for epoch_index in range(args.num_epochs):\n",
    "    train_state[\"epoch_index\"] = epoch_index\n",
    "    \n",
    "    # Training dataset\n",
    "    dataset.set_split(\"train\")\n",
    "    batch_gen = gen_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    clf.train()\n",
    "    for batch_index, batch_dict in enumerate(batch_gen):\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute output\n",
    "        y_pred = clf(inputs=batch_dict[\"features\"])\n",
    "        # Compute loss\n",
    "        loss = sequence_loss(y_pred, batch_dict[\"labels\"], mask_index)\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch-running_loss) / (batch_index+1)\n",
    "        # Compute gradients using loss\n",
    "        loss.backward()\n",
    "        # Use optimizer to take gradient step\n",
    "        optimizer.step()\n",
    "        # Compute accuracy\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict[\"labels\"], mask_index)\n",
    "        running_acc += (acc_batch-running_acc) / (batch_index+1)\n",
    "        \n",
    "        # Update the bar\n",
    "        train_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "        train_bar.update()\n",
    "            \n",
    "    train_state[\"train_loss\"].append(running_loss)\n",
    "    train_state[\"train_acc\"].append(running_acc)\n",
    "    \n",
    "    # Validation dataset\n",
    "    dataset.set_split(\"valid\")\n",
    "    batch_gen = gen_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    clf.eval()\n",
    "    for batch_index, batch_dict in enumerate(batch_gen):\n",
    "        # Compute output\n",
    "        y_pred = clf(inputs=batch_dict[\"features\"])\n",
    "        # Compute loss\n",
    "        loss = sequence_loss(y_pred, batch_dict[\"labels\"], mask_index)\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch-running_loss) / (batch_index+1)\n",
    "        # Compute accuracy\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict[\"labels\"], mask_index)\n",
    "        running_acc += (acc_batch-running_acc) / (batch_index+1)\n",
    "        \n",
    "    train_state[\"valid_loss\"].append(running_loss)\n",
    "    train_state[\"valid_acc\"].append(running_acc)\n",
    "    \n",
    "    train_state = update_train_state(args=args, model=clf, train_state=train_state)\n",
    "\n",
    "    scheduler.step(train_state['valid_loss'][-1])\n",
    "\n",
    "    train_bar.n = 0\n",
    "    epoch_bar.update()\n",
    "    \n",
    "    if train_state['stop_early']:\n",
    "        break\n",
    "train_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "train_bar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73552d25",
   "metadata": {},
   "source": [
    "#### Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7372ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_split(\"test\")\n",
    "batch_gen = gen_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "running_loss = 0.0\n",
    "running_acc = 0.0\n",
    "clf.eval()\n",
    "for batch_index, batch_dict in enumerate(batch_gen):\n",
    "    # Compute output\n",
    "    y_pred = clf(inputs=batch_dict[\"features\"])\n",
    "    # Compute loss\n",
    "    loss = sequence_loss(y_pred, batch_dict[\"labels\"], mask_index)\n",
    "    loss_batch = loss.item()\n",
    "    running_loss += (loss_batch-running_loss) / (batch_index+1)\n",
    "    # Compute accuracy\n",
    "    acc_batch = compute_accuracy(y_pred, batch_dict[\"labels\"], mask_index)\n",
    "    running_acc += (acc_batch-running_acc) / (batch_index+1)\n",
    "        \n",
    "train_state[\"test_loss\"] = running_loss\n",
    "train_state[\"test_acc\"] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a8a3da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 20.013228387600414\n",
      "Test loss    : 2.373884081840515\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test accuracy: {train_state['test_acc']}\")\n",
    "print(f\"Test loss    : {train_state['test_loss']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8a021d",
   "metadata": {},
   "source": [
    "### Comparison of the RNN and the GRU classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa7a9e5",
   "metadata": {},
   "source": [
    "Unfortunate I was unable to delve too deep into the results of the two classifiers, but in this case, the GRU classifier performed somewhat better.\n",
    "\n",
    "That isn't say that it performed well, which it certainly didn't (accuracy of ~21%), but its better than the RNNs ~3%.\n",
    "\n",
    "Similary to the comparison done in Task 1, I didn't experiment much with the hyperparameters, and there is likely much room for improvement. Same with the arcitechture of the RNN/GRU model. As far as I could tell, there was no notable improvement with increasing the number of layers in the RNN/GRU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
